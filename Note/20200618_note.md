# Weekly16
> Part1：深度學習簡介 \
> Part2：Pytorch 預訓練模型 \
> Part3：Pytorch 強化式學習
## Part1
1. 深度學習：深沉的神經網路
* MNIST 手寫數字辨識
    - 黑白輸入只要一個像素(1byte)
    - 彩色需要3byte以上
    - 捲積層 conv -> relu 層 -> 池化層pool 層 -> conv 層 -> relu 層 -> ...-> fc 全連接層 -> softmax 函數 輸出
* CNN 捲積神經網路：透過不斷捲積，層層把區塊放大後學習特徵，通常分析圖片
* RNN 循環神經網路：可以記住一些歷史訊息，網路的訓練是給他數據讓機器學期，為了預測下一個出現，通常分析音訊數據化，比如：語音或文字
* GAN 生成對抗網路：隨機數生成一個作品，Generator會隨機而生成，Discriminator會判斷哪些為真，那些為生成數據，經學習後再傳回給Generator，使隨機慢慢生成象真實數據
* bert：類似克漏字填充，主要做到可以回填
## Part2、Part3
1. 使用 Pytorch、venv 虛擬環境執行
* [alexnet.py](https://gitlab.com/ccckmit/ai2/-/blob/master/python/11-deepLearning/01-pretrained/01-torchvision/01-classify/alexnet.py)
* [segment.py](https://gitlab.com/ccckmit/ai2/-/blob/master/python/11-deepLearning/01-pretrained/01-torchvision/02-semantic/segment.py)
* [torchtext_classify.py](https://gitlab.com/ccckmit/ai2/-/blob/master/python/11-deepLearning/01-pretrained/02-torchtext/01-classify1/torchtext_classify.py)
.
.
.

---
資料來源&參考：
- [深度學習的捲積神經網路](https://www.slideshare.net/ccckmit/javascript-nodejs?fbclid=IwAR1jJZm21D1jIRqrGjadpREAE6eV2U5q5GoYKRnSVsqYNGFNcKmggMmLQfU)
- [
GitLab：ai2/python/11-deepLearning/07-gan/01-gan_mnist/samples](https://gitlab.com/ccckmit/ai2/-/tree/master/python/11-deepLearning/07-gan/01-gan_mnist/samples?fbclid=IwAR2AQDaoh370ytXTWweZG4YAR9tyYeAxRLmpU1ZjlsqnXUCtHN_k2N1l7A0)
- [ConvNetJS](https://cs.stanford.edu/people/karpathy/convnetjs/?fbclid=IwAR08EQm7ypKmdg3W9kLcwxx5fPN2Fd7shKH-lb5AF1BFUHD3p6JV_cHZB1Y)
- [深度學習的RNN/LSTM循環神經網路](https://www.slideshare.net/ccckmit/rnn-lstm-77568016?fbclid=IwAR2qNNONYzEMwviPN4g5h8Ntx7xU5bC1O3ufiby7HaBQ31_oSo2Krzm1FpA)
- [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/?fbclid=IwAR2fJ5_rdPjZscG9iI16lUt6Meo8jVx1UXkyf9LjhPNTbsOlVuZJ2jLvoFo)
- [生成对抗网络 (GAN)](https://morvanzhou.github.io/tutorials/machine-learning/ML-intro/2-6-GAN/?fbclid=IwAR3dKIP_tibcNEWL_Us_BU2ocjBW9s-qmcVWYSN2DPk2w9XvFWQKZmebTa4)
- [生成對抗網絡](https://zh.wikipedia.org/wiki/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C?fbclid=IwAR3V_B6_r39Rg5fkjDvYVBYv6NpXfz6MrEnrfmxOJHRRNldmGNK0N0XSgOI)
- [進擊的 BERT：NLP 界的巨人之力與遷移學習](https://leemeng.tw/attack_on_bert_transfer_learning_in_nlp.html?fbclid=IwAR0qOe8Yb4VsHpI63uEXG3ct22LELYqdCUtZ17GIT40lWOIACKt04p39k54)
- []()
